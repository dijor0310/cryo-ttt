{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from copy import deepcopy\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from utils.utils import set_seed\n",
    "from utils.subtomos import reassemble_subtomograms_v2\n",
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as FT\n",
    "import mrcfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import build_dataset\n",
    "# from models import build_model\n",
    "from models.denoiseg import Denoiseg\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "with initialize(version_base=None, config_path=\"configs/\"):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "cfg = OmegaConf.merge(cfg, cfg.method)\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "criterion = DiceLoss(sigmoid=True)\n",
    "scoring_fn = DiceMetric()\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss(reduction='none')\n",
    "# mse_loss = nn.L1Loss(reduction='none')\n",
    "device = torch.device(\"cuda:3\")\n",
    "device_1 = torch.device(\"cuda:1\")\n",
    "bcm = BinaryConfusionMatrix().to(device)\n",
    "\n",
    "dataset = build_dataset(cfg, test=True)\n",
    "\n",
    "test_dataset, test_val_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 2, 2])\n",
    "print(len(test_dataset), len(test_val_dataset))\n",
    "# test_dataset = dataset\n",
    "# test_val_dataset = build_dataset(cfg, val=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=cfg.load_num_workers,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    persistent_workers=cfg.persistent_workers,\n",
    ")            \n",
    "test_val_loader = torch.utils.data.DataLoader(\n",
    "    test_val_dataset,\n",
    "    num_workers=cfg.load_num_workers,\n",
    "    batch_size=2,\n",
    "    persistent_workers=cfg.persistent_workers,\n",
    ")            \n",
    "\n",
    "test_entire_gt = mrcfile.read(cfg.test_entire_gt)\n",
    "test_entire_gt = torch.Tensor(test_entire_gt)[None, None, ...]\n",
    "\n",
    "model = Denoiseg.load_from_checkpoint(cfg.ckpt_path, map_location=device, config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(gt, pred, pred_refined, id):\n",
    "\n",
    "    batch_size = gt.size(0)\n",
    "    for sample in range(batch_size):\n",
    "        # gt = gt.sum(dim=0)\n",
    "        # pred = pred.sum(dim=0)\n",
    "        # pred_refined = pred.sum(dim=0)\n",
    "        diff = torch.abs(pred_refined - pred)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "        # Display each image with its corresponding title\n",
    "        images = [gt, pred, pred_refined, diff]\n",
    "        titles = [\"gt\", \"pred\", \"pred after ttt\", \"diff\"]\n",
    "\n",
    "        for ax, img, title in zip(axes, images, titles):\n",
    "            ax.imshow(img[sample, 0].cpu().detach().sum(dim=0))\n",
    "            ax.set_title(title)\n",
    "            ax.axis(\"off\")  # Hide axes for better visual appearance\n",
    "\n",
    "        # Adjust layout and save the figure\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Path(\"./plots\") / f\"{id[sample]}.png\", bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Define the transformation and its inverse\n",
    "def random_transform_with_inverse(tensor):\n",
    "    transformation = random.choice(['rotate', 'mirror'])\n",
    "    inverse_function = None\n",
    "\n",
    "    if transformation == 'rotate':\n",
    "        k = random.choice([1, 2, 3])\n",
    "        transformed_tensor = torch.rot90(tensor, k, [3, 4])\n",
    "        inverse_function = lambda x: torch.rot90(x, -k, [3, 4])\n",
    "\n",
    "    elif transformation == 'mirror':\n",
    "        axis = random.choice([3, 4])\n",
    "        transformed_tensor = torch.flip(tensor, [axis])\n",
    "        inverse_function = lambda x: torch.flip(x, [axis])\n",
    "\n",
    "    return transformed_tensor, inverse_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def logging(run, dice_loss, dice_coef, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(x_hat, x, mask):\n",
    "    loss = mse_loss(x_hat, x) * mask\n",
    "    return loss.sum() / mask.sum()\n",
    "\n",
    "def dice_from_conf_matrix(conf_matrix):\n",
    "    return 2 * conf_matrix[1, 1] / (2 * conf_matrix[1, 1] + conf_matrix[0, 1] + conf_matrix[1, 0])\n",
    "\n",
    "def get_grad_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    return total_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_one_batch(x_masked, x_unmasked, mask, model, n_steps, lr, momentum, validation=False, y=None, id=None, test_val_loader=None):\n",
    "    \n",
    "    # run = wandb.init(project=\"cryo-ttt-ttt\", name=id[0])\n",
    "    val_losses = []\n",
    "    val_dices = []\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), momentum=momentum, lr=lr)\n",
    "    # run.log(\n",
    "    #     {\n",
    "    #         # \"train/subtomo\": wandb.Image(x_unmasked[0, :, 128]),\n",
    "    #         \"train/seg_overall_gt\": wandb.Image(y[0, :].sum(dim=0).float()),\n",
    "    #     }\n",
    "    # )\n",
    "    \n",
    "\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat, x_hat = model.model(x_masked)\n",
    "\n",
    "        loss = masked_mse_loss(x_hat, x_unmasked, mask)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_grad_norm = get_grad_norm(model.model)\n",
    "        if validation:\n",
    "            model.eval()\n",
    "            y_hat, _ = model.model(x_unmasked)\n",
    "            x_unmasked_aug, inverse_fn = random_transform_with_inverse(x_unmasked)\n",
    "            y_hat_aug, _ = model.model(x_unmasked_aug)\n",
    "            y_hat_aug = inverse_fn(y_hat_aug)\n",
    "\n",
    "            consistency = criterion((y_hat.sigmoid() > 0.5).int(), (y_hat_aug.sigmoid() > 0.5).int()).detach().item()\n",
    "            dice_loss = criterion(y_hat, y).detach().item()\n",
    "            dice_coef = scoring_fn((y_hat.sigmoid() > 0.5).int(), y).detach().item()\n",
    "            print(f\"loss: {dice_loss:.3f} | coef: {dice_coef:.3f} | mse_loss: {loss.item():.3f}\")\n",
    "            val_losses.append(dice_loss)\n",
    "            val_dices.append(dice_coef)\n",
    "\n",
    "            for batch in test_val_loader:\n",
    "                x_masked_val, x_unmasked_val = batch[\"image\"].to(device), batch[\"unmasked_image\"].to(device)\n",
    "                y_out, id_val = batch[\"label\"].to(device), batch[\"id\"]\n",
    "                mask_val = batch[\"mask\"].to(device)\n",
    "\n",
    "                _, x_hat_val = model.model(x_masked_val)\n",
    "\n",
    "                val_mse_loss = masked_mse_loss(x_hat_val, x_unmasked_val, mask_val)\n",
    "                # val_dice_loss = criterion(y_hat_val, y_out).detach().item()\n",
    "                # run.log({\n",
    "                #     \"val/mse_loss\": val_mse_loss,\n",
    "                #     \"val/dice_loss\": val_dice_loss,\n",
    "                # })\n",
    "\n",
    "            model.train()\n",
    "        \n",
    "        slice_to_log = torch.argmax(y[0, 0].sum(dim=(-1, -2)))\n",
    "        # run.log({\n",
    "        #     \"train/mse_loss\": loss.item(),\n",
    "        #     \"train/dice_loss\": dice_loss,\n",
    "        #     \"train/dice\": dice_coef,\n",
    "        #     \"step\": step,\n",
    "        #     # \"train/seg\": wandb.Image(torch.cat((y[0, :, 128].float(), y_hat.sigmoid()[0, :, 128]), dim=1)),\n",
    "        #     # \"train/denoised\": wandb.Image(torch.cat((x_unmasked[0, :, 128], x_hat[0, :, 128]), dim=1)),\n",
    "        #     \"train/subtomo\": wandb.Image(x_unmasked[0, :, slice_to_log]),\n",
    "        #     \"train/denoised\": wandb.Image(x_hat[0, :, slice_to_log]),\n",
    "        #     \"train/seg_gt\": wandb.Image(FT.to_pil_image((y[0, :, slice_to_log] * 255).to(torch.uint8), mode=\"L\")),\n",
    "        #     \"train/seg\": wandb.Image(FT.to_pil_image((y_hat.sigmoid()[0, :, slice_to_log] * 255).to(torch.uint8), mode=\"L\")),\n",
    "        #     \"train/mask\": wandb.Image(FT.to_pil_image((mask[0, :, slice_to_log] * 255).to(torch.uint8), mode=\"L\")),\n",
    "        #     \"val/mse_loss\": val_mse_loss.item(),\n",
    "        #     \"train/grad_norm\": train_grad_norm,\n",
    "        #     \"train/intensity_var\": x_hat.var(),\n",
    "        #     \"train/seg_overall_gt\": wandb.Image(FT.to_pil_image(y[0, :].sum(dim=1).to(torch.uint8), mode=\"L\")),\n",
    "        #     \"train/seg_overall\": wandb.Image(FT.to_pil_image((y_hat.sigmoid()[0, :] > 0.5).sum(dim=1).to(torch.uint8), mode=\"L\")),\n",
    "        #     \"train/consistency\": consistency,\n",
    "        # })\n",
    "        x_masked, mask = dataset.generate_mask(deepcopy(x_unmasked[0, 0].cpu()))\n",
    "        x_masked, mask = torch.Tensor(x_masked)[None, None, ...].to(device), torch.Tensor(mask)[None, None, ...].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    y_hat, _ = model.model(x_unmasked)\n",
    "    \n",
    "    # run.finish()\n",
    "    return y_hat, val_losses, val_dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt(model, test_loader, n_steps, lr, momentum, validation=False, test_val_loader=None):\n",
    "    \n",
    "    dices = []\n",
    "    dices_raw = []\n",
    "    conf_matrix = torch.zeros(2, 2)\n",
    "    conf_matrix_raw = torch.zeros(2, 2)\n",
    "\n",
    "    val_losses = []\n",
    "    val_dices = []\n",
    "    # run = wandb.init(project=\"cryo-ttt-ttt\", name=\"first\")\n",
    "    # run.finish()\n",
    "\n",
    "    y_hats = []\n",
    "    y_hats_raw = []\n",
    "    centers = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "\n",
    "        # model.load_state_dict(torch.load(cfg.ckpt_path))\n",
    "\n",
    "        x_masked, x_unmasked = batch[\"image\"].to(device), batch[\"unmasked_image\"].to(device)\n",
    "        y_out, id = batch[\"label\"].to(device), batch[\"id\"]\n",
    "        mask = batch[\"mask\"].to(device)\n",
    "\n",
    "        model.eval()\n",
    "        y_hat_raw, _ = model.model(x_unmasked)\n",
    "        # optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)\n",
    "        y_hat, val_step_losses, val_step_dices = ttt_one_batch(\n",
    "            x_masked,\n",
    "            x_unmasked,\n",
    "            mask, deepcopy(model), n_steps, lr, momentum, validation=validation, y=y_out, id=id, test_val_loader=test_val_loader)\n",
    "\n",
    "        y_hats.append(y_hat.squeeze().detach().cpu().numpy())\n",
    "        y_hats_raw.append(y_hat_raw.squeeze().detach().cpu().numpy())\n",
    "        center = id[0].split(\"_\")[-1][1:-1]\n",
    "        center = center.split(\" \")\n",
    "        center = [int(elem) for elem in center]\n",
    "        print(f\"Center: {center}\")\n",
    "        centers.append(center)\n",
    "\n",
    "        val_losses.append(val_step_losses)\n",
    "        val_dices.append(val_step_dices)\n",
    "\n",
    "        dice_score = 1 - criterion(y_hat, y_out).detach().item()\n",
    "        dice_score_raw = 1 - criterion(y_hat_raw, y_out).detach().item()\n",
    "        conf_matrix += bcm(y_hat.sigmoid()[:, :, 12:-12, 12:-12, 12:-12], y_out[:, :, 12:-12, 12:-12, 12:-12]).detach().cpu()\n",
    "        conf_matrix_raw += bcm(y_hat_raw.sigmoid()[:, :, 12:-12, 12:-12, 12:-12], y_out[:, :, 12:-12, 12:-12, 12:-12]).detach().cpu()\n",
    "\n",
    "        print(f\"bilo: {dice_score_raw:.3f}, stalo: {dice_score:.3f}\")\n",
    "        dices.append(dice_score)\n",
    "        dices_raw.append(dice_score_raw)\n",
    "    \n",
    "    print(f\"Mean Dice after ttt: {sum(dices) / len(dices)}\")\n",
    "    print(f\"Macro Dice after ttt: {dice_from_conf_matrix(conf_matrix)}\")\n",
    "    print(f\"Mean Dice raw: {sum(dices_raw) / len(dices_raw)}\")\n",
    "    print(f\"Macro Dice raw: {dice_from_conf_matrix(conf_matrix_raw)}\")\n",
    "\n",
    "    reassembled_pred = reassemble_subtomograms_v2(y_hats, centers, test_entire_gt.shape[2:], 256, 12)\n",
    "    reassembled_pred = (torch.Tensor(reassembled_pred).sigmoid() > 0.5).to(torch.uint8)[None, None, ...]\n",
    "\n",
    "    reassembled_pred_raw = reassemble_subtomograms_v2(y_hats_raw, centers, test_entire_gt.shape[2:], 256, 12)\n",
    "    reassembled_pred_raw = (torch.Tensor(reassembled_pred_raw).sigmoid() > 0.5).to(torch.uint8)[None, None, ...]\n",
    "\n",
    "\n",
    "    very_macro_dice = scoring_fn(reassembled_pred_raw, test_entire_gt).item()\n",
    "    very_macro_dice_after = scoring_fn(reassembled_pred, test_entire_gt).item()\n",
    "\n",
    "    run = wandb.init(project=\"cryo-ttt-ttt\", name=\"last\")\n",
    "    print(cfg.test_data_root_dir)\n",
    "    run.log({\n",
    "        \"macro_dice\": dice_from_conf_matrix(conf_matrix_raw),\n",
    "        \"macro_dice_after\": dice_from_conf_matrix(conf_matrix),\n",
    "        \"mean_dice\": sum(dices_raw) / len(dices_raw),\n",
    "        \"mean_dice_after\": sum(dices) / len(dices),\n",
    "        \"very_macro_dice\": very_macro_dice,\n",
    "        \"very_macro_dice_after\": very_macro_dice_after,\n",
    "    })\n",
    "    run.finish()\n",
    "\n",
    "    return dices, dices_raw, conf_matrix, conf_matrix_raw, val_losses, val_dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.694 | coef: 0.375 | mse_loss: 0.697\n",
      "loss: 0.683 | coef: 0.379 | mse_loss: 0.656\n",
      "loss: 0.684 | coef: 0.370 | mse_loss: 0.620\n",
      "loss: 0.688 | coef: 0.360 | mse_loss: 0.582\n",
      "loss: 0.689 | coef: 0.354 | mse_loss: 0.556\n",
      "loss: 0.693 | coef: 0.346 | mse_loss: 0.530\n",
      "loss: 0.699 | coef: 0.339 | mse_loss: 0.504\n",
      "loss: 0.707 | coef: 0.331 | mse_loss: 0.472\n",
      "loss: 0.718 | coef: 0.325 | mse_loss: 0.446\n",
      "loss: 0.724 | coef: 0.318 | mse_loss: 0.430\n",
      "Center: [128, 824, 360]\n",
      "bilo: 0.289, stalo: 0.276\n",
      "loss: 0.540 | coef: 0.476 | mse_loss: 0.697\n",
      "loss: 0.525 | coef: 0.490 | mse_loss: 0.663\n",
      "loss: 0.512 | coef: 0.502 | mse_loss: 0.632\n",
      "loss: 0.503 | coef: 0.510 | mse_loss: 0.607\n",
      "loss: 0.496 | coef: 0.517 | mse_loss: 0.587\n",
      "loss: 0.491 | coef: 0.521 | mse_loss: 0.570\n",
      "loss: 0.488 | coef: 0.523 | mse_loss: 0.550\n",
      "loss: 0.486 | coef: 0.524 | mse_loss: 0.530\n",
      "loss: 0.486 | coef: 0.523 | mse_loss: 0.506\n",
      "loss: 0.488 | coef: 0.522 | mse_loss: 0.483\n",
      "Center: [128, 128, 128]\n",
      "bilo: 0.449, stalo: 0.512\n",
      "loss: 0.430 | coef: 0.590 | mse_loss: 0.669\n",
      "loss: 0.417 | coef: 0.601 | mse_loss: 0.637\n",
      "loss: 0.404 | coef: 0.613 | mse_loss: 0.602\n",
      "loss: 0.392 | coef: 0.624 | mse_loss: 0.576\n",
      "loss: 0.385 | coef: 0.630 | mse_loss: 0.559\n",
      "loss: 0.379 | coef: 0.635 | mse_loss: 0.541\n",
      "loss: 0.375 | coef: 0.639 | mse_loss: 0.521\n",
      "loss: 0.372 | coef: 0.641 | mse_loss: 0.499\n",
      "loss: 0.372 | coef: 0.642 | mse_loss: 0.475\n",
      "loss: 0.374 | coef: 0.642 | mse_loss: 0.451\n",
      "Center: [128, 128, 592]\n",
      "bilo: 0.561, stalo: 0.626\n",
      "loss: 0.666 | coef: 0.335 | mse_loss: 0.820\n",
      "loss: 0.664 | coef: 0.338 | mse_loss: 0.806\n",
      "loss: 0.661 | coef: 0.340 | mse_loss: 0.794\n",
      "loss: 0.333 | coef: 0.674 | mse_loss: 0.769\n",
      "loss: 0.268 | coef: 0.737 | mse_loss: 0.684\n",
      "loss: 0.237 | coef: 0.767 | mse_loss: 0.663\n",
      "loss: 0.217 | coef: 0.786 | mse_loss: 0.653\n",
      "loss: 0.204 | coef: 0.798 | mse_loss: 0.635\n",
      "loss: 0.196 | coef: 0.807 | mse_loss: 0.608\n",
      "loss: 0.190 | coef: 0.812 | mse_loss: 0.590\n",
      "Center: [128, 360, 824]\n",
      "bilo: 0.333, stalo: 0.810\n",
      "loss: 0.807 | coef: 0.194 | mse_loss: 0.825\n",
      "loss: 0.790 | coef: 0.210 | mse_loss: 0.804\n",
      "loss: 0.763 | coef: 0.236 | mse_loss: 0.769\n",
      "loss: 0.708 | coef: 0.292 | mse_loss: 0.691\n",
      "loss: 0.666 | coef: 0.337 | mse_loss: 0.624\n",
      "loss: 0.596 | coef: 0.411 | mse_loss: 0.612\n",
      "loss: 0.541 | coef: 0.463 | mse_loss: 0.603\n",
      "loss: 0.487 | coef: 0.514 | mse_loss: 0.589\n",
      "loss: 0.400 | coef: 0.613 | mse_loss: 0.576\n",
      "loss: 0.326 | coef: 0.715 | mse_loss: 0.567\n",
      "Center: [128, 128, 824]\n",
      "bilo: 0.180, stalo: 0.674\n",
      "loss: 0.505 | coef: 0.530 | mse_loss: 0.683\n",
      "loss: 0.424 | coef: 0.607 | mse_loss: 0.629\n",
      "loss: 0.377 | coef: 0.651 | mse_loss: 0.592\n",
      "loss: 0.350 | coef: 0.677 | mse_loss: 0.571\n",
      "loss: 0.337 | coef: 0.689 | mse_loss: 0.552\n",
      "loss: 0.331 | coef: 0.696 | mse_loss: 0.531\n",
      "loss: 0.329 | coef: 0.699 | mse_loss: 0.511\n",
      "loss: 0.331 | coef: 0.701 | mse_loss: 0.488\n",
      "loss: 0.334 | coef: 0.701 | mse_loss: 0.467\n",
      "loss: 0.336 | coef: 0.700 | mse_loss: 0.440\n",
      "Center: [128, 360, 592]\n",
      "bilo: 0.402, stalo: 0.664\n",
      "loss: 0.414 | coef: 0.593 | mse_loss: 0.680\n",
      "loss: 0.382 | coef: 0.624 | mse_loss: 0.653\n",
      "loss: 0.351 | coef: 0.655 | mse_loss: 0.626\n",
      "loss: 0.326 | coef: 0.680 | mse_loss: 0.605\n",
      "loss: 0.308 | coef: 0.697 | mse_loss: 0.588\n",
      "loss: 0.297 | coef: 0.708 | mse_loss: 0.564\n",
      "loss: 0.291 | coef: 0.714 | mse_loss: 0.543\n",
      "loss: 0.287 | coef: 0.718 | mse_loss: 0.519\n",
      "loss: 0.285 | coef: 0.721 | mse_loss: 0.497\n",
      "loss: 0.284 | coef: 0.722 | mse_loss: 0.472\n",
      "Center: [128, 128, 360]\n",
      "bilo: 0.563, stalo: 0.716\n",
      "loss: 0.990 | coef: 0.016 | mse_loss: 0.971\n",
      "loss: 0.956 | coef: 0.061 | mse_loss: 0.824\n",
      "loss: 0.938 | coef: 0.076 | mse_loss: 0.668\n",
      "loss: 0.940 | coef: 0.067 | mse_loss: 0.601\n",
      "loss: 0.943 | coef: 0.062 | mse_loss: 0.587\n",
      "loss: 0.945 | coef: 0.060 | mse_loss: 0.578\n",
      "loss: 0.946 | coef: 0.059 | mse_loss: 0.540\n",
      "loss: 0.950 | coef: 0.057 | mse_loss: 0.479\n",
      "loss: 0.948 | coef: 0.056 | mse_loss: 0.529\n",
      "loss: 0.947 | coef: 0.055 | mse_loss: 0.449\n",
      "Center: [128, 824, 128]\n",
      "bilo: 0.005, stalo: 0.053\n",
      "loss: 0.526 | coef: 0.481 | mse_loss: 0.788\n",
      "loss: 0.352 | coef: 0.655 | mse_loss: 0.782\n",
      "loss: 0.307 | coef: 0.700 | mse_loss: 0.796\n",
      "loss: 0.297 | coef: 0.710 | mse_loss: 0.775\n",
      "loss: 0.290 | coef: 0.716 | mse_loss: 0.755\n",
      "loss: 0.292 | coef: 0.714 | mse_loss: 0.715\n",
      "loss: 0.299 | coef: 0.707 | mse_loss: 0.697\n",
      "loss: 0.306 | coef: 0.701 | mse_loss: 0.673\n",
      "loss: 0.302 | coef: 0.705 | mse_loss: 0.648\n",
      "loss: 0.300 | coef: 0.707 | mse_loss: 0.623\n",
      "Center: [128, 824, 824]\n",
      "bilo: 0.569, stalo: 0.700\n",
      "loss: 0.530 | coef: 0.478 | mse_loss: 0.686\n",
      "loss: 0.476 | coef: 0.531 | mse_loss: 0.648\n",
      "loss: 0.433 | coef: 0.574 | mse_loss: 0.610\n",
      "loss: 0.402 | coef: 0.604 | mse_loss: 0.583\n",
      "loss: 0.378 | coef: 0.627 | mse_loss: 0.565\n",
      "loss: 0.361 | coef: 0.644 | mse_loss: 0.544\n",
      "loss: 0.350 | coef: 0.655 | mse_loss: 0.518\n",
      "loss: 0.344 | coef: 0.662 | mse_loss: 0.494\n",
      "loss: 0.340 | coef: 0.667 | mse_loss: 0.470\n",
      "loss: 0.338 | coef: 0.671 | mse_loss: 0.451\n",
      "Center: [128, 360, 360]\n",
      "bilo: 0.432, stalo: 0.662\n",
      "loss: 0.350 | coef: 0.670 | mse_loss: 0.648\n",
      "loss: 0.337 | coef: 0.682 | mse_loss: 0.628\n",
      "loss: 0.324 | coef: 0.694 | mse_loss: 0.605\n",
      "loss: 0.315 | coef: 0.703 | mse_loss: 0.585\n",
      "loss: 0.308 | coef: 0.709 | mse_loss: 0.567\n",
      "loss: 0.304 | coef: 0.713 | mse_loss: 0.549\n",
      "loss: 0.303 | coef: 0.714 | mse_loss: 0.526\n",
      "loss: 0.305 | coef: 0.715 | mse_loss: 0.506\n",
      "loss: 0.307 | coef: 0.715 | mse_loss: 0.488\n",
      "loss: 0.308 | coef: 0.714 | mse_loss: 0.469\n",
      "Center: [128, 824, 592]\n",
      "bilo: 0.643, stalo: 0.692\n",
      "loss: 0.674 | coef: 0.379 | mse_loss: 0.731\n",
      "loss: 0.601 | coef: 0.449 | mse_loss: 0.670\n",
      "loss: 0.575 | coef: 0.469 | mse_loss: 0.615\n",
      "loss: 0.556 | coef: 0.485 | mse_loss: 0.585\n",
      "loss: 0.547 | coef: 0.491 | mse_loss: 0.561\n",
      "loss: 0.543 | coef: 0.495 | mse_loss: 0.535\n",
      "loss: 0.543 | coef: 0.495 | mse_loss: 0.506\n",
      "loss: 0.550 | coef: 0.492 | mse_loss: 0.478\n",
      "loss: 0.558 | coef: 0.488 | mse_loss: 0.454\n",
      "loss: 0.562 | coef: 0.483 | mse_loss: 0.429\n",
      "Center: [128, 592, 592]\n",
      "bilo: 0.255, stalo: 0.438\n",
      "loss: 0.707 | coef: 0.313 | mse_loss: 0.737\n",
      "loss: 0.659 | coef: 0.360 | mse_loss: 0.687\n",
      "loss: 0.618 | coef: 0.400 | mse_loss: 0.631\n",
      "loss: 0.594 | coef: 0.421 | mse_loss: 0.596\n",
      "loss: 0.581 | coef: 0.433 | mse_loss: 0.569\n",
      "loss: 0.569 | coef: 0.443 | mse_loss: 0.547\n",
      "loss: 0.560 | coef: 0.452 | mse_loss: 0.521\n",
      "loss: 0.554 | coef: 0.458 | mse_loss: 0.492\n",
      "loss: 0.553 | coef: 0.462 | mse_loss: 0.464\n",
      "loss: 0.552 | coef: 0.465 | mse_loss: 0.443\n",
      "Center: [128, 592, 128]\n",
      "bilo: 0.255, stalo: 0.448\n",
      "loss: 0.276 | coef: 0.737 | mse_loss: 0.831\n",
      "loss: 0.246 | coef: 0.764 | mse_loss: 0.747\n",
      "loss: 0.239 | coef: 0.770 | mse_loss: 0.720\n",
      "loss: 0.240 | coef: 0.768 | mse_loss: 0.701\n",
      "loss: 0.243 | coef: 0.764 | mse_loss: 0.688\n",
      "loss: 0.247 | coef: 0.759 | mse_loss: 0.679\n",
      "loss: 0.251 | coef: 0.755 | mse_loss: 0.669\n",
      "loss: 0.255 | coef: 0.751 | mse_loss: 0.656\n",
      "loss: 0.259 | coef: 0.747 | mse_loss: 0.643\n",
      "loss: 0.263 | coef: 0.744 | mse_loss: 0.628\n",
      "Center: [128, 592, 824]\n",
      "bilo: 0.194, stalo: 0.737\n",
      "loss: 0.611 | coef: 0.398 | mse_loss: 0.676\n",
      "loss: 0.578 | coef: 0.431 | mse_loss: 0.658\n",
      "loss: 0.546 | coef: 0.462 | mse_loss: 0.630\n",
      "loss: 0.522 | coef: 0.485 | mse_loss: 0.606\n",
      "loss: 0.506 | coef: 0.501 | mse_loss: 0.587\n",
      "loss: 0.496 | coef: 0.512 | mse_loss: 0.566\n",
      "loss: 0.490 | coef: 0.517 | mse_loss: 0.546\n",
      "loss: 0.486 | coef: 0.521 | mse_loss: 0.523\n",
      "loss: 0.485 | coef: 0.523 | mse_loss: 0.498\n",
      "loss: 0.485 | coef: 0.524 | mse_loss: 0.475\n",
      "Center: [128, 360, 128]\n",
      "bilo: 0.370, stalo: 0.515\n",
      "Mean Dice after ttt: 0.568169367313385\n",
      "Macro Dice after ttt: 0.6821755170822144\n",
      "Mean Dice raw: 0.3666198054949443\n",
      "Macro Dice raw: 0.34654560685157776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdijor0310\u001b[0m (\u001b[33mcryo-diyor\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspaces/cryo/cryo-ttt/src/wandb/run-20250131_111537-ndytfytm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cryo-diyor/cryo-ttt-ttt/runs/ndytfytm' target=\"_blank\">last</a></strong> to <a href='https://wandb.ai/cryo-diyor/cryo-ttt-ttt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cryo-diyor/cryo-ttt-ttt' target=\"_blank\">https://wandb.ai/cryo-diyor/cryo-ttt-ttt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cryo-diyor/cryo-ttt-ttt/runs/ndytfytm' target=\"_blank\">https://wandb.ai/cryo-diyor/cryo-ttt-ttt/runs/ndytfytm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>macro_dice</td><td>▁</td></tr><tr><td>macro_dice_after</td><td>▁</td></tr><tr><td>mean_dice</td><td>▁</td></tr><tr><td>mean_dice_after</td><td>▁</td></tr><tr><td>very_macro_dice</td><td>▁</td></tr><tr><td>very_macro_dice_after</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>macro_dice</td><td>0.34655</td></tr><tr><td>macro_dice_after</td><td>0.68218</td></tr><tr><td>mean_dice</td><td>0.36662</td></tr><tr><td>mean_dice_after</td><td>0.56817</td></tr><tr><td>very_macro_dice</td><td>0.3464</td></tr><tr><td>very_macro_dice_after</td><td>0.68178</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">last</strong> at: <a href='https://wandb.ai/cryo-diyor/cryo-ttt-ttt/runs/ndytfytm' target=\"_blank\">https://wandb.ai/cryo-diyor/cryo-ttt-ttt/runs/ndytfytm</a><br> View project at: <a href='https://wandb.ai/cryo-diyor/cryo-ttt-ttt' target=\"_blank\">https://wandb.ai/cryo-diyor/cryo-ttt-ttt</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250131_111537-ndytfytm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dices = ttt(model, test_loader, 10, 0.001, 0.9, validation=True, test_val_loader=test_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_hats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my_hats\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_hats' is not defined"
     ]
    }
   ],
   "source": [
    "y_hats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Define the transformation and its inverse\n",
    "def random_transform_with_inverse(tensor):\n",
    "    transformation = random.choice(['rotate', 'mirror'])\n",
    "    inverse_function = None\n",
    "\n",
    "    if transformation == 'rotate':\n",
    "        k = random.choice([1, 2, 3])\n",
    "        transformed_tensor = torch.rot90(tensor, k, [3, 4])\n",
    "        inverse_function = lambda x: torch.rot90(x, -k, [3, 4])\n",
    "\n",
    "    elif transformation == 'mirror':\n",
    "        axis = random.choice([3, 4])\n",
    "        transformed_tensor = torch.flip(tensor, [axis])\n",
    "        inverse_function = lambda x: torch.flip(x, [axis])\n",
    "\n",
    "    return transformed_tensor, inverse_function\n",
    "\n",
    "# # Step 2: Example usage in the flow\n",
    "# B, D, H, W = 2, 10, 64, 64\n",
    "# original_tensor = torch.rand((B, 1, D, H, W))\n",
    "\n",
    "# # Apply random transformation and get the inverse function\n",
    "# augmented_tensor, inverse_function = random_transform_with_inverse(original_tensor)\n",
    "\n",
    "# # Mock neural network model\n",
    "# class MockModel(torch.nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         return x  # Mock output, replace with actual model inference\n",
    "\n",
    "# model = MockModel()\n",
    "\n",
    "# # Step 3: Pass through the neural network\n",
    "# original_segmentation = model(original_tensor)\n",
    "# augmented_segmentation = model(augmented_tensor)\n",
    "\n",
    "# # Step 4: Apply inverse transformation to the augmented segmentation\n",
    "# transformed_back_segmentation = inverse_function(augmented_segmentation)\n",
    "\n",
    "# # Step 5: Compare original and transformed-back segmentation\n",
    "# consistency_loss = F.mse_loss(original_segmentation, transformed_back_segmentation)\n",
    "\n",
    "# print(\"Consistency Loss:\", consistency_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_consistency_one_batch(x, model, n_steps, lr, momentum):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), momentum=momentum, lr=lr)\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        x1, inverse_fn_1 = random_transform_with_inverse(x)\n",
    "        x2, inverse_fn_2 = random_transform_with_inverse(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y1, _ = model.model(x1)\n",
    "        y2, _ = model.model(x2)\n",
    "        y1 = inverse_fn_1(y1)\n",
    "        y2 = inverse_fn_2(y2)\n",
    "\n",
    "        # loss = F.binary_cross_entropy_with_logits(y1, y2)\n",
    "        loss = criterion(y1, (y2.sigmoid() > 0.5).int())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"Step {step}: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    y_hat, _ = model.model(x)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_consistency(model, test_loader, n_steps, lr, momentum):\n",
    "    \n",
    "    dices = []\n",
    "    dices_raw = []\n",
    "    conf_matrix = torch.zeros(2, 2)\n",
    "    conf_matrix_raw = torch.zeros(2, 2)\n",
    "    for batch in test_loader:\n",
    "\n",
    "        # model.load_state_dict(torch.load(cfg.ckpt_path))\n",
    "\n",
    "        x_unmasked = batch[\"unmasked_image\"].to(device)\n",
    "        y_out, id = batch[\"label\"].to(device), batch[\"id\"]\n",
    "\n",
    "        model.eval()\n",
    "        y_hat_raw, _ = model.model(x_unmasked)\n",
    "\n",
    "        y_hat = ttt_consistency_one_batch(x_unmasked, deepcopy(model), n_steps, lr, momentum)\n",
    "\n",
    "        dice_score = 1 - criterion(y_hat, y_out).detach().item()\n",
    "        dice_score_raw = 1 - criterion(y_hat_raw, y_out).detach().item()\n",
    "        conf_matrix += bcm(y_hat.sigmoid(), y_out).detach().cpu()\n",
    "        conf_matrix_raw += bcm(y_hat_raw.sigmoid(), y_out).detach().cpu()\n",
    "\n",
    "        print(f\"bilo: {dice_score_raw:.3f}, stalo: {dice_score:.3f}\")\n",
    "\n",
    "        dices.append(dice_score)\n",
    "        dices_raw.append(dice_score_raw)\n",
    "    \n",
    "    print(f\"Mean Dice after ttt: {sum(dices) / len(dices)}\")\n",
    "    print(f\"Macro Dice after ttt: {dice_from_conf_matrix(conf_matrix)}\")\n",
    "    print(f\"Mean Dice raw: {sum(dices_raw) / len(dices_raw)}\")\n",
    "    print(f\"Macro Dice raw: {dice_from_conf_matrix(conf_matrix_raw)}\")\n",
    "\n",
    "    return dices, dices_raw, conf_matrix, conf_matrix_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dices, dices_raw = ttt_consistency(model, test_loader, 10, 0.001, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_combined(model, test_loader, n_steps_1, n_steps_2, lr, momentum):\n",
    "\n",
    "    dices_raw = []\n",
    "    dices_1 = []\n",
    "    dices_2 = []\n",
    "    conf_matrix_raw = torch.zeros(2, 2)\n",
    "    conf_matrix_1 = torch.zeros(2, 2)\n",
    "    conf_matrix_2 = torch.zeros(2, 2)\n",
    "    for batch in test_loader:\n",
    "\n",
    "        # model.load_state_dict(torch.load(cfg.ckpt_path))\n",
    "\n",
    "        x_masked, x_unmasked = batch[\"image\"].to(device), batch[\"unmasked_image\"].to(device)\n",
    "        y_out, id = batch[\"label\"].to(device), batch[\"id\"]\n",
    "        mask = batch[\"mask\"].to(device)\n",
    "\n",
    "        model.eval()\n",
    "        y_hat_raw, _ = model.model(x_unmasked)\n",
    "        dice_score_raw = 1 - criterion(y_hat_raw, y_out).detach().item()\n",
    "        conf_matrix_raw += bcm(y_hat_raw.sigmoid(), y_out).detach().cpu()\n",
    "\n",
    "        # optimizer = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=0.9)\n",
    "        model_temp = deepcopy(model)\n",
    "\n",
    "        y_hat = ttt_one_batch(x_masked, x_unmasked, mask, model_temp, n_steps_1, lr, momentum)\n",
    "        dice_score_1 = 1 - criterion(y_hat, y_out).detach().item()\n",
    "        conf_matrix_1 += bcm(y_hat.sigmoid(), y_out).detach().cpu()\n",
    "        plot(y_out, y_hat_raw, y_hat, id)\n",
    "        print(id)\n",
    "\n",
    "        y_hat = ttt_consistency_one_batch(x_unmasked, model_temp, n_steps_2, lr, momentum)\n",
    "        dice_score_2 = 1 - criterion(y_hat, y_out).detach().item()\n",
    "        conf_matrix_2 += bcm(y_hat.sigmoid(), y_out).detach().cpu()\n",
    "\n",
    "        print(f\"bilo: {dice_score_raw:.3f}, stalo: {dice_score_1:.3f} stalo: {dice_score_2:.3f}\")\n",
    "        # print(dice_from_conf_matrix(conf_matrix_1))\n",
    "        # print(scoring_fn(y_hat.sigmoid(), y_out).detach().item())\n",
    "        # print(dice_from_conf_matrix(conf_matrix_2))\n",
    "        dices_raw.append(dice_score_raw)\n",
    "        dices_1.append(dice_score_1)\n",
    "        dices_2.append(dice_score_2)\n",
    "        # plot(y_out, y_hat_raw, y_hat, id)\n",
    "    \n",
    "    print(sum(dices_2) / len(dices_2))\n",
    "    print(sum(dices_1) / len(dices_1))\n",
    "    print(sum(dices_raw) / len(dices_raw))\n",
    "    print()\n",
    "    print(f\"Macro Dice 2: {dice_from_conf_matrix(conf_matrix_2)}\")\n",
    "    print(f\"Macro Dice 1: {dice_from_conf_matrix(conf_matrix_1)}\")\n",
    "    print(f\"Macro Dice: {dice_from_conf_matrix(conf_matrix_raw)}\")\n",
    "    return dices_1, dices_2, dices_raw, conf_matrix_1, conf_matrix_2, conf_matrix_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ttt_combined(model, test_loader, 10, 10, 0.001, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_summed_one_batch(x_masked, x_unmasked, mask, model, n_steps, lr, momentum):\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), momentum=momentum, lr=lr)\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        x1, inverse_fn_1 = random_transform_with_inverse(x_unmasked)\n",
    "        x2, inverse_fn_2 = random_transform_with_inverse(x_unmasked)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y1, _ = model.model(x1)\n",
    "        y2, _ = model.model(x2)\n",
    "        y1 = inverse_fn_1(y1)\n",
    "        y2 = inverse_fn_2(y2)\n",
    "\n",
    "        _, x_denoised = model.model(x_masked)\n",
    "\n",
    "        loss = criterion(y1, (y2.sigmoid() > 0.5).int()) + masked_mse_loss(x_denoised, x_unmasked, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_hat, _ = model.model(x_unmasked)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_summed(model, test_loader, n_steps, lr, momentum):\n",
    "\n",
    "    dices_raw = []\n",
    "    dices_1 = []\n",
    "    conf_matrix_raw = torch.zeros(2, 2)\n",
    "    conf_matrix_1 = torch.zeros(2, 2)\n",
    "\n",
    "    for batch in test_loader:\n",
    "\n",
    "        x_masked, x_unmasked = batch[\"image\"].to(device), batch[\"unmasked_image\"].to(device)\n",
    "        y_out, id = batch[\"label\"].to(device), batch[\"id\"]\n",
    "        mask = batch[\"mask\"].to(device)\n",
    "\n",
    "        model.eval()\n",
    "        y_hat_raw, _ = model.model(x_unmasked)\n",
    "        dice_score_raw = 1 - criterion(y_hat_raw, y_out).detach().item()\n",
    "        conf_matrix_raw += bcm(y_hat_raw.sigmoid(), y_out).detach().cpu()\n",
    "        \n",
    "        y_hat = ttt_summed_one_batch(\n",
    "            x_masked,\n",
    "            x_unmasked,\n",
    "            mask,\n",
    "            deepcopy(model),\n",
    "            n_steps,\n",
    "            lr,\n",
    "            momentum\n",
    "        )\n",
    "        dice_score_1 = 1 - criterion(y_hat, y_out).detach().item()\n",
    "        conf_matrix_1 += bcm(y_hat.sigmoid(), y_out).detach().cpu()\n",
    "\n",
    "        print(f\"bilo: {dice_score_raw:.3f}, stalo: {dice_score_1:.3f}\")\n",
    "\n",
    "        dices_raw.append(dice_score_raw)\n",
    "        dices_1.append(dice_score_1)\n",
    "    \n",
    "    print(sum(dices_1) / len(dices_1))\n",
    "    print(sum(dices_raw) / len(dices_raw))\n",
    "    print()\n",
    "    print(f\"Macro Dice 1: {dice_from_conf_matrix(conf_matrix_1)}\")\n",
    "    print(f\"Macro Dice: {dice_from_conf_matrix(conf_matrix_raw)}\")\n",
    "    return dices_1, dices_raw, conf_matrix_1, conf_matrix_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ttt_summed(model, test_loader, 20, 0.0005, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ttt(model, test_loader, 100, 0.001, 0.9, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_two_line_plots(lines_plot1, lines_plot2, save_path):\n",
    "    \"\"\"\n",
    "    Plots two subplots, each containing multiple lines. Each line is provided\n",
    "    as a list of y-values, and x-values are assumed to be integer steps (0, 1, 2, ...).\n",
    "    \n",
    "    Parameters:\n",
    "    - lines_plot1: List of lists, where each list contains the y-values for a line in plot 1.\n",
    "    - lines_plot2: List of lists, where each list contains the y-values for a line in plot 2.\n",
    "    \"\"\"\n",
    "    # Generate x-coordinates (just integers, same length as y)\n",
    "    x = list(range(len(lines_plot1[0])))  # Assuming all lines have the same length\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot lines for the first plot\n",
    "    for y in lines_plot1:\n",
    "        ax1.plot(x, y)\n",
    "    ax1.plot(x, np.array(lines_plot1).mean(axis=0), linestyle=\"--\", linewidth=2)\n",
    "    ax1.set_title('Losses')\n",
    "\n",
    "    # Plot lines for the second plot\n",
    "    for y in lines_plot2:\n",
    "        ax2.plot(x, y)\n",
    "    ax2.plot(x, np.array(lines_plot2).mean(axis=0), linestyle=\"--\", linewidth=2)\n",
    "    ax2.set_title('Scores')\n",
    "\n",
    "    fig.savefig(save_path)\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scienceplots\n",
    "plt.style.use('science')\n",
    "\n",
    "plot_two_line_plots(out[-2], out[-1], \"./plots/graphs/ttt_steps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_dices = []\n",
    "outs = []\n",
    "prefix = Path(\"/mnt/hdd_pool_zion/userdata/diyor/data/deepict/training/12-12-trimmed-256-b20/work/testing_data\")\n",
    "tomo_paths = [\n",
    "    # \"vpp_train/001\",\n",
    "    \"vpp_train/002\",\n",
    "    # \"vpp_train/003\",\n",
    "    \"vpp_train/004\",\n",
    "    # \"vpp_train/005\",\n",
    "    \"vpp_train/006\",\n",
    "    \"vpp_test/007\",\n",
    "    \"vpp_val/008\",\n",
    "    # \"vpp_val/009\",\n",
    "    \"vpp_val/010\",\n",
    "    # \"def_train/026\",\n",
    "    # \"def_train/030\",\n",
    "    # \"def_train/034\",\n",
    "    # \"def_val/037\",\n",
    "    # \"def_test/041\",\n",
    "]\n",
    "for path in tomo_paths:\n",
    "\n",
    "    macro_dices_tomo = []\n",
    "    cfg.test_data_root_dir = prefix / path\n",
    "    print(f\"Tomo {path}\")\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        build_dataset(cfg, test=True),\n",
    "        num_workers=cfg.load_num_workers,\n",
    "        batch_size=1,\n",
    "        persistent_workers=cfg.persistent_workers,\n",
    "    )\n",
    "\n",
    "    for steps in range(0, 40, 10):\n",
    "        out = ttt(model, test_loader, steps, 0.001, 0.9, validation=False)\n",
    "        macro_dices_tomo.append(dice_from_conf_matrix(out[2]))\n",
    "        # outs.append(out)\n",
    "    \n",
    "    macro_dices.append(macro_dices_tomo)\n",
    "    # losses.append(1 - out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scienceplots\n",
    "plt.style.use(['science', 'nature'])\n",
    "for elem in macro_dices:\n",
    "    plt.plot(list(range(0, 40, 10)), elem)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, 30)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "prefix = Path(\"/mnt/hdd_pool_zion/userdata/diyor/data/deepict/training/12-12-trimmed-256-b20/work/testing_data\")\n",
    "tomo_paths = [\n",
    "    # \"vpp_train/001\",\n",
    "    # \"vpp_train/002\",\n",
    "    # \"vpp_train/003\",\n",
    "    # \"vpp_train/004\",\n",
    "    # \"vpp_train/005\",\n",
    "    # \"vpp_train/006\",\n",
    "    # \"vpp_test/007\",\n",
    "    # \"vpp_val/008\",\n",
    "    # \"vpp_val/009\",\n",
    "    # \"vpp_val/010\",\n",
    "    \"def_train/026\",\n",
    "    \"def_train/030\",\n",
    "    \"def_train/034\",\n",
    "    \"def_val/037\",\n",
    "    \"def_test/041\",\n",
    "]\n",
    "x = np.arange(len(tomo_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_table = {\n",
    "    # \"baseline\": [],\n",
    "    # \"denoising_ttt\": [],\n",
    "    # \"rotation_ttt\": [],\n",
    "    # \"combined_ttt\": [],\n",
    "    \"summed_ttt\": [],\n",
    "}\n",
    "\n",
    "for path in tomo_paths:\n",
    "\n",
    "    cfg.test_data_root_dir = prefix / path\n",
    "    print(f\"Tomo {path}\")\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        build_dataset(cfg, test=True),\n",
    "        num_workers=cfg.load_num_workers,\n",
    "        batch_size=1,\n",
    "        persistent_workers=cfg.persistent_workers,\n",
    "    )\n",
    "\n",
    "    dices_denoising, dices_raw, conf_m_denoising, conf_m_raw = ttt(model, test_loader, 20, 0.001, 0.9)\n",
    "    # dices_rotation, _, conf_m_rotation, _ = ttt_consistency(model, test_loader, 10, 0.001, 0.9)\n",
    "    # _, dices_combined, _, _, conf_m_combined, _ = ttt_combined(model, test_loader, 10, 10, 0.001, 0.9)\n",
    "\n",
    "\n",
    "    # cached_table[\"baseline\"].append(dice_from_conf_matrix(conf_m_raw))\n",
    "    # cached_table[\"denoising_ttt\"].append(dice_from_conf_matrix(conf_m_denoising))\n",
    "    # cached_table[\"rotation_ttt\"].append(dice_from_conf_matrix(conf_m_rotation))\n",
    "    # cached_table[\"combined_ttt\"].append(dice_from_conf_matrix(conf_m_combined))\n",
    "\n",
    "# Save table to alter the plotting in the future\n",
    "# with open(\"cache/ttt_results_vpp_vpp_macro.pkl\", \"wb\") as handle:\n",
    "    # pickle.dump(cached_table, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import pickle\n",
    "\n",
    "names = {\n",
    "    \"baseline\": \"Baseline (Segmentation + Noise2Void)\",\n",
    "    \"denoising_ttt\": \"Noise2Void TTT [10 steps]\",\n",
    "    \"rotation_ttt\": \"Rotation/Flip Consistency TTT [10 steps]\",\n",
    "    \"combined_ttt\": \"Noise2Void + Rotation/Flip Consistency TTT [10 + 10 steps]\"\n",
    "}\n",
    "plt.style.use(['science', 'nature'])\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "with open(\"cache/ttt_results_vpp_def_macro.pkl\", \"rb\") as handle:\n",
    "    cached_table = pickle.load(handle)\n",
    "table = {}\n",
    "for k, v in cached_table.items():\n",
    "    table[names[k]] = cached_table[k]\n",
    "\n",
    "width = 0.2  # the width of the bars\n",
    "multiplier = -0.5\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "# fig, ax = plt.subplots()\n",
    "fig.set_size_inches(6.5, 2.786)\n",
    "\n",
    "for attribute, measurement in table.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.axhline(np.array(measurement).mean(), color=rects.patches[0].get_facecolor(), linestyle=\"--\", zorder=0)\n",
    "    # ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "x_labels = [\"def/\" + tomo.split(\"/\")[-1] for tomo in tomo_paths]\n",
    "\n",
    "ax.set_ylabel('Dice coefficient')\n",
    "# ax.set_title('Trained on DEF, TTT on VPP')\n",
    "ax.set_xticks(x + width, x_labels)\n",
    "ax.legend(loc='upper left', ncols=2)\n",
    "ax.set_ylim(0, 1)\n",
    "# ax.yaxis.grid(True)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"plots/graphs/vpp_def_macro_65.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import pickle\n",
    "\n",
    "x = [0]\n",
    "names = {\n",
    "    \"baseline\": \"Baseline (Segmentation + Noise2Void)\",\n",
    "    \"denoising_ttt\": \"Noise2Void TTT [10 steps]\",\n",
    "    \"rotation_ttt\": \"Rotation/Flip Consistency TTT [10 steps]\",\n",
    "    \"combined_ttt\": \"Noise2Void + Rotation/Flip Consistency TTT [10 + 10 steps]\"\n",
    "}\n",
    "plt.style.use(['science', 'nature'])\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "\n",
    "with open(\"cache/ttt_results_def_def_macro.pkl\", \"rb\") as handle:\n",
    "    cached_table = pickle.load(handle)\n",
    "table = {}\n",
    "for k, v in cached_table.items():\n",
    "    table[names[k]] = cached_table[k]\n",
    "\n",
    "with open(\"cache/ttt_results_vpp_vpp_macro.pkl\", \"rb\") as handle:\n",
    "    cached_table = pickle.load(handle)\n",
    "table1 = {}\n",
    "for k, v in cached_table.items():\n",
    "    table1[names[k]] = cached_table[k]\n",
    "\n",
    "width = 0.2  # the width of the bars\n",
    "multiplier = -0.5\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, layout='constrained')\n",
    "# fig, ax = plt.subplots()\n",
    "fig.set_size_inches(6.5, 2.786)\n",
    "\n",
    "for attribute, measurement in table.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax[0][0].bar(x + offset, measurement, width, label=attribute)\n",
    "    ax[0][0].axhline(np.array(measurement).mean(), color=rects.patches[0].get_facecolor(), linestyle=\"--\", zorder=0)\n",
    "    # ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "for attribute, measurement in table.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax[0][1].bar(x + offset, measurement, width, label=attribute)\n",
    "    ax[0][1].axhline(np.array(measurement).mean(), color=rects.patches[0].get_facecolor(), linestyle=\"--\", zorder=0)\n",
    "    # ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "# x_labels = [\"def/\" + tomo.split(\"/\")[-1] for tomo in tomo_paths]\n",
    "x_labels = [\"def/041\"]\n",
    "ax[0][0].set_ylabel('Dice coefficient')\n",
    "# ax.set_title('Trained on DEF, TTT on VPP')\n",
    "ax[0][0].set_xticks(x + width, x_labels)\n",
    "# ax[0][0].legend(loc='upper left', ncols=2)\n",
    "ax[0][0].set_ylim(0, 1)\n",
    "\n",
    "# x_labels = [\"vpp/\" + tomo.split(\"/\")[-1] for tomo in tomo_paths]\n",
    "# ax[0][0].set_ylabel('Dice coefficient')\n",
    "# ax.set_title('Trained on DEF, TTT on VPP')\n",
    "x_labels = [\"vpp/007\"]\n",
    "ax[0][0].set_xticks(x + width, x_labels)\n",
    "# ax[0][0].legend(loc='upper left', ncols=2)\n",
    "ax[0][0].set_ylim(0, 1)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center')\n",
    "# ax.yaxis.grid(True)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"plots/graphs/vpp_def_macro_65.png\", dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
